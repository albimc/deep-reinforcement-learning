{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import inspect\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from ddpg_agent import Agent\n",
    "# from ddpg_model import Actor, Critic\n",
    "# from ddpg_interact import Interact\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Agents\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "We are using a 20 Agents environment\n",
    "\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis_20/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print (brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.07849999824538827\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters 1e-3\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor 1e-4\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay 0\n",
    "TIMESTEPS_WAIT = 1     # how many timesteps to wait forlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, seed=1, timestamp=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        # self.memory.add(state, action, reward, next_state, done)\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestamp += 1\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        oktolearn = (self.timestamp % TIMESTEPS_WAIT) == 0\n",
    "        okbatchsize = len(self.memory) > BATCH_SIZE\n",
    "        # print(oktolearn, okbatchsize)\n",
    "        if (okbatchsize & oktolearn):\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True, eps=1.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample(epsilon=eps)\n",
    "        # action = (action + 1.0) / 2.0\n",
    "        # return np.clip(action, 0, 1)\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)  # Grad clipping\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.actor_local.parameters(), 1)  # Grad clipping\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        # print('Noise Process: mean {}, drift {}, stdev {} '.format(mu, theta, sigma))\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self, epsilon=1.):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        # dx = self.theta * (self.mu - x) + self.sigma * epsilon * np.array([np.random.randn() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * epsilon * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Actor - Critic Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # self.fc2_drop = nn.Dropout(p=0.05)          # Dropout\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.leaky_relu(self.fc1(state))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        # x = self.fc2_drop(x)                        # Dropout   \n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return F.tanh(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        # self.fc2_drop = nn.Dropout(p=0.05)                     # Dropout\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        # x = self.fc2_drop(x)                                     # Dropout\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interact(env, agent, brain_name, num_agents, train_mode=True, add_noise=True,\n",
    "             n_episodes=1000, max_t=400, eps_start=1.0, eps_end=0.1, eps_decay=0.999, n_window=40, tgt_score=30):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    print('\\rRunning for {} episodes with target score of {}'.format(n_episodes, tgt_score))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('\\rUsing {}'.format(device))\n",
    "    start = time.time()\n",
    "\n",
    "    # Initialise\n",
    "    scores = []                                             # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=n_window)                  # last 100 scores\n",
    "    eps = eps_start                                         # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]    # reset the environment\n",
    "        states = env_info.vector_observations                    # get the current state\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            # Agent Selects Action #\n",
    "            actions = agent.act(states, add_noise=add_noise, eps=eps)     # Agent action\n",
    "            # Agent Performs Action #\n",
    "            env_info = env.step(actions)[brain_name]                # Environment reacts\n",
    "            next_states = env_info.vector_observations           # get the next state\n",
    "            rewards = env_info.rewards                           # get the reward\n",
    "            dones = env_info.local_done                          # see if episode has finished\n",
    "            # Agent Observes State #\n",
    "            agent.step(states, actions, rewards, next_states, dones)    # Agent observes new state\n",
    "            states = next_states                                     # roll over the state to next time step\n",
    "            score += rewards                                        # update the score\n",
    "            if any(dones):\n",
    "                break\n",
    "        scores_window.append(np.mean(score))        # save most recent score\n",
    "        scores.append(np.mean(score))               # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)           # decrease epsilon\n",
    "        # Periodic Check\n",
    "        if i_episode % n_window == 0:\n",
    "            end = time.time()\n",
    "            print('\\nElapsed time {:.1f}'.format((end - start)/60), 'Steps {}'.format(i_episode*max_t), 'Agent Steps {}'.format(agent.timestamp), end=\"\")\n",
    "            print('\\nEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            print('\\nEpisode {}\\tLast Action:'.format(i_episode), actions[0], end=\"\")\n",
    "            print('\\nEpisode {}\\tLast Epsilon: {:.2f}'.format(i_episode, eps), end=\"\")\n",
    "            torch.save(agent.actor_local.state_dict(), './Data/checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), './Data/checkpoint_critic.pth')\n",
    "        if np.mean(scores_window) >= tgt_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-n_window, np.mean(scores_window)))\n",
    "            break\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, seed=1, timestamp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUFFER_SIZE 100000, BATCH_SIZE 128, GAMMA 0.99, TAU 0.001,\n",
      "\t\t LR_ACTOR 0.001, LR_CRITIC 0.001, WEIGHT_DECAY 0, TIMESTEPS_WAIT 1\n",
      "Agent: (state_size, action_size, num_agents, seed=1, timestamp=1)\n",
      "Noise: (size, seed, mu=0.0, theta=0.15, sigma=0.2)\n"
     ]
    }
   ],
   "source": [
    "print('BUFFER_SIZE {}, BATCH_SIZE {}, GAMMA {}, TAU {},\\n\\t\\t LR_ACTOR {}, LR_CRITIC {}, WEIGHT_DECAY {}, TIMESTEPS_WAIT {}'.format(BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR_ACTOR, LR_CRITIC, WEIGHT_DECAY, TIMESTEPS_WAIT))\n",
    "print('Agent:',inspect.signature(agent.__init__))\n",
    "print('Noise:',inspect.signature(agent.noise.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 1000 episodes with target score of 40\n",
      "Using cuda:0\n",
      "\n",
      "Elapsed time 2.2 Steps 10000 Agent Steps 10001\n",
      "Episode 10\tAverage Score: 0.84\n",
      "Episode 10\tLast Action: [ 1.         -0.77756613 -0.8648717  -0.74689776]\n",
      "Episode 10\tLast Epsilon: 0.99\n",
      "Elapsed time 4.4 Steps 20000 Agent Steps 20001\n",
      "Episode 20\tAverage Score: 1.02\n",
      "Episode 20\tLast Action: [-0.8827485   0.61080134  0.82851726  1.        ]\n",
      "Episode 20\tLast Epsilon: 0.98\n",
      "Elapsed time 6.7 Steps 30000 Agent Steps 30001\n",
      "Episode 30\tAverage Score: 2.27\n",
      "Episode 30\tLast Action: [ 0.9380423   1.         -0.44523275 -0.16775091]\n",
      "Episode 30\tLast Epsilon: 0.97\n",
      "Elapsed time 8.9 Steps 40000 Agent Steps 40001\n",
      "Episode 40\tAverage Score: 7.81\n",
      "Episode 40\tLast Action: [-0.41972497 -1.         -0.5940593  -0.29119608]\n",
      "Episode 40\tLast Epsilon: 0.96\n",
      "Elapsed time 11.1 Steps 50000 Agent Steps 50001\n",
      "Episode 50\tAverage Score: 13.43\n",
      "Episode 50\tLast Action: [-1.         1.        -0.8974703  1.       ]\n",
      "Episode 50\tLast Epsilon: 0.95\n",
      "Elapsed time 13.5 Steps 60000 Agent Steps 60001\n",
      "Episode 60\tAverage Score: 21.96\n",
      "Episode 60\tLast Action: [0.30665866 0.8776307  0.15077505 1.        ]\n",
      "Episode 60\tLast Epsilon: 0.94\n",
      "Elapsed time 15.9 Steps 70000 Agent Steps 70001\n",
      "Episode 70\tAverage Score: 25.20\n",
      "Episode 70\tLast Action: [ 0.9817548  -0.82425326  1.          0.41033345]\n",
      "Episode 70\tLast Epsilon: 0.93\n",
      "Elapsed time 18.3 Steps 80000 Agent Steps 80001\n",
      "Episode 80\tAverage Score: 27.43\n",
      "Episode 80\tLast Action: [-0.90317637  1.         -0.39557946 -0.72573465]\n",
      "Episode 80\tLast Epsilon: 0.92\n",
      "Elapsed time 20.8 Steps 90000 Agent Steps 90001\n",
      "Episode 90\tAverage Score: 30.69\n",
      "Episode 90\tLast Action: [ 1.         -0.8375502   0.90828305  0.55365294]\n",
      "Episode 90\tLast Epsilon: 0.91\n",
      "Elapsed time 23.2 Steps 100000 Agent Steps 100001\n",
      "Episode 100\tAverage Score: 36.81\n",
      "Episode 100\tLast Action: [-0.19000635  1.         -0.4145753  -0.69440854]\n",
      "Episode 100\tLast Epsilon: 0.90\n",
      "Elapsed time 25.6 Steps 110000 Agent Steps 110001\n",
      "Episode 110\tAverage Score: 37.04\n",
      "Episode 110\tLast Action: [ 0.31755581  0.83306634  0.99181074 -0.22597352]\n",
      "Episode 110\tLast Epsilon: 0.90\n",
      "Elapsed time 27.9 Steps 120000 Agent Steps 120001\n",
      "Episode 120\tAverage Score: 36.56\n",
      "Episode 120\tLast Action: [ 1.         -0.8335258   0.7314     -0.06973549]\n",
      "Episode 120\tLast Epsilon: 0.89\n",
      "Elapsed time 30.3 Steps 130000 Agent Steps 130001\n",
      "Episode 130\tAverage Score: 36.79\n",
      "Episode 130\tLast Action: [ 0.95365     0.47781396  0.6819475  -0.42004895]\n",
      "Episode 130\tLast Epsilon: 0.88\n",
      "Elapsed time 32.8 Steps 140000 Agent Steps 140001\n",
      "Episode 140\tAverage Score: 37.21\n",
      "Episode 140\tLast Action: [ 0.06774865 -0.542021   -0.19226065  0.08588678]\n",
      "Episode 140\tLast Epsilon: 0.87\n",
      "Elapsed time 35.2 Steps 150000 Agent Steps 150001\n",
      "Episode 150\tAverage Score: 36.13\n",
      "Episode 150\tLast Action: [ 0.4042979   0.67137545 -0.22738338 -0.11705352]\n",
      "Episode 150\tLast Epsilon: 0.86\n",
      "Elapsed time 37.6 Steps 160000 Agent Steps 160001\n",
      "Episode 160\tAverage Score: 36.79\n",
      "Episode 160\tLast Action: [-0.40936604 -0.3634456   0.93629766  0.748597  ]\n",
      "Episode 160\tLast Epsilon: 0.85\n",
      "Elapsed time 40.0 Steps 170000 Agent Steps 170001\n",
      "Episode 170\tAverage Score: 37.65\n",
      "Episode 170\tLast Action: [ 0.66196895 -0.25089884  0.37053907 -0.41119578]\n",
      "Episode 170\tLast Epsilon: 0.84\n",
      "Elapsed time 42.4 Steps 180000 Agent Steps 180001\n",
      "Episode 180\tAverage Score: 37.20\n",
      "Episode 180\tLast Action: [ 0.63527757  1.          0.57542056 -0.00171933]\n",
      "Episode 180\tLast Epsilon: 0.84\n",
      "Elapsed time 44.9 Steps 190000 Agent Steps 190001\n",
      "Episode 190\tAverage Score: 38.04\n",
      "Episode 190\tLast Action: [ 0.9032633  -0.07732286 -0.39687705  0.31982505]\n",
      "Episode 190\tLast Epsilon: 0.83\n",
      "Elapsed time 47.2 Steps 200000 Agent Steps 200001\n",
      "Episode 200\tAverage Score: 37.39\n",
      "Episode 200\tLast Action: [ 1.          0.07493753  0.36062157 -0.5136535 ]\n",
      "Episode 200\tLast Epsilon: 0.82\n",
      "Elapsed time 49.6 Steps 210000 Agent Steps 210001\n",
      "Episode 210\tAverage Score: 38.20\n",
      "Episode 210\tLast Action: [-0.89517415  1.         -0.1212305   0.49216497]\n",
      "Episode 210\tLast Epsilon: 0.81\n",
      "Elapsed time 52.0 Steps 220000 Agent Steps 220001\n",
      "Episode 220\tAverage Score: 37.64\n",
      "Episode 220\tLast Action: [ 0.63322    -0.48122638  0.7080861  -0.471617  ]\n",
      "Episode 220\tLast Epsilon: 0.80\n",
      "Elapsed time 54.4 Steps 230000 Agent Steps 230001\n",
      "Episode 230\tAverage Score: 37.63\n",
      "Episode 230\tLast Action: [-0.6526864  -0.9075624   0.43575782  0.9351971 ]\n",
      "Episode 230\tLast Epsilon: 0.79\n",
      "Elapsed time 56.8 Steps 240000 Agent Steps 240001\n",
      "Episode 240\tAverage Score: 37.01\n",
      "Episode 240\tLast Action: [-0.7192448   1.         -0.10590224 -0.28730774]\n",
      "Episode 240\tLast Epsilon: 0.79\n",
      "Elapsed time 59.2 Steps 250000 Agent Steps 250001\n",
      "Episode 250\tAverage Score: 37.31\n",
      "Episode 250\tLast Action: [-1.          1.         -0.56750786  0.9188745 ]\n",
      "Episode 250\tLast Epsilon: 0.78\n",
      "Elapsed time 61.6 Steps 260000 Agent Steps 260001\n",
      "Episode 260\tAverage Score: 37.32\n",
      "Episode 260\tLast Action: [-0.88894385 -0.6509898  -0.62996966 -0.2830088 ]\n",
      "Episode 260\tLast Epsilon: 0.77\n",
      "Elapsed time 64.0 Steps 270000 Agent Steps 270001\n",
      "Episode 270\tAverage Score: 37.93\n",
      "Episode 270\tLast Action: [-0.670715   -0.56132686 -0.23452501  0.22937877]\n",
      "Episode 270\tLast Epsilon: 0.76"
     ]
    }
   ],
   "source": [
    "scores = Interact(env, agent, brain_name, num_agents, train_mode=True, add_noise=True, n_episodes=1000,\n",
    "                  max_t=1000, eps_start=1, eps_end=0.2, eps_decay=0.999, n_window=10, tgt_score=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(scores).to_pickle(\"./Data/scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_win = scores[0:200]\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_win)), scores_win)\n",
    "plt.axhline(y=30, color='r', linestyle='dashed')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('./Plots/Average_Score.pdf')\n",
    "fig.savefig('./Plots/Average_Score.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY SMART AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('./Data/checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('./Data/checkpoint_critic.pth'))\n",
    "\n",
    "## Play \n",
    "for i in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    for j in range(200):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations    # get the next state\n",
    "        rewards = env_info.rewards                    # get the reward\n",
    "        dones = env_info.local_done                   # see if episode has finished\n",
    "        scores += rewards                             # update the score\n",
    "        states = next_states                          # roll over the state to next time step\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
